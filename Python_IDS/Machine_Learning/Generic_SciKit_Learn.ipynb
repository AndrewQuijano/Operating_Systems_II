{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Columbia REU 2018 - Classifiers\n",
    "- Currently the plan is to build an indoor localization classier from multiple weak learners.\n",
    "- 1) Wi-Fi localization classifier\n",
    "- 2) Bluetooth localization classifier\n",
    "- 3) Miscelleanous classifier (altitude, luminosity, magnetic field, etc.). This probably will be the weakest classifier because the only feature that should change the most is altitude.\n",
    "- 4) Confirm with Henning wether this should be room only or room/building.\n",
    "\n",
    "We will be using the following classifiers as suggested by Gabriel Young.\n",
    "i)   K-NN (Try 3-NN, it supposedly is an unsually powerful classifier).\n",
    "ii)  penalized logistic regression\n",
    "iii) random forest \n",
    "iv)  neural network\n",
    "v)   radial basis kernel \n",
    " \n",
    "The purpose of this code was that it was written to be generic as possible as to be used for all other Machine Learning Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Loading all Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import brier_score_loss, classification_report, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale, LabelEncoder, StandardScaler\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# How to tune\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# LDA/QDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# Neural Network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# SVM\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Should read this...\n",
    "# https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions to Load your data set and build CV Test\n",
    "- Will Consider making a global Label Encoder to already support plotting ahead of time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only for this specific case...\n",
    "def read_data_set(filename, isnumeric=False):\n",
    "    x = np.genfromtxt(filename, delimiter=',', skip_header=1)\n",
    "    if isnumeric:\n",
    "        y = x[:, 0]\n",
    "    else:\n",
    "        y = np.genfromtxt(filename, delimiter=',', skip_header=1, dtype=str, usecols=0)\n",
    "\n",
    "    # x must delete first column which is the label\n",
    "    x = x[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_cv_set(training_set, test_set, percentile=0.2):\n",
    "    row = np.shape(training_set)[0]\n",
    "    col = np.shape(training_set)[1]\n",
    "    sample_idx = random.sample(range(row), int(percentile * row))\n",
    "\n",
    "    # Get your CV data\n",
    "    cv_train = training_set[sample_idx[:], 0:col]\n",
    "    cv_test = test_set[sample_idx[:]]\n",
    "\n",
    "    # Remove CV data from original\n",
    "    set_diff = np.setdiff1d(np.arange(row), sample_idx)\n",
    "\n",
    "    training_set = training_set[set_diff[:], 0:col]\n",
    "    test_set = test_set[set_diff[:]]\n",
    "    return training_set, test_set, cv_train, cv_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Machine Learning situations are ok with getting first n answers as your guess. In Indoor Localization, an emergency worker will try to look at another room if the first guess is wrong.\n",
    "- KNN\n",
    "- Naive Bayes \n",
    "- Random Forest\n",
    "- Combined Classifier\n",
    "# DO NOT have a decision function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top(clf, test_x, test_y, extra_rooms=1):\n",
    "    # Get your list...\n",
    "    # Sort it such that highest probabilities come first...\n",
    "    # https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value\n",
    "    # To print highest first, set reverse=True\n",
    "    probability_dict = []\n",
    "    for i in range(len(test_y)):\n",
    "        if hasattr(clf, 'decision_function'):\n",
    "            probability_dict[i] = sorted([(v, k) for k, v in probability_dict[i].items()], reverse=True)\n",
    "        else:\n",
    "             probability_dict.append(dict(zip(clf.classes_, clf.predict_proba(test_x)[i])))\n",
    "        probability_dict[i] = sorted([(v, k) for k, v in probability_dict[i].items()], reverse=True)\n",
    "\n",
    "    success = 0\n",
    "    # Let us say test the first 3 rooms? See if it matches!\n",
    "    for i in range(len(test_y)):\n",
    "        # print(probability_dict[i])\n",
    "        for j in range(extra_rooms):\n",
    "            if probability_dict[i][j][1] == test_y[i]:\n",
    "                success = success + 1\n",
    "                break\n",
    "    print(\"Test Error for \" + str(extra_rooms) +\" Rooms: \" + str(success/len(test_y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot CV Error from GridSearch/RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid_search(cv_results, grid_param, name_param):\n",
    "    # Get Test Scores Mean and std for each grid search\n",
    "    scores_mean = cv_results['mean_test_score']\n",
    "    scores_mean = np.array(scores_mean).reshape(len(grid_param))\n",
    "\n",
    "    # scores_sd = cv_results['std_test_score']\n",
    "    # scores_sd = np.array(scores_sd).reshape(len(grid_param))\n",
    "\n",
    "    # Plot Grid search scores\n",
    "    _, ax = plt.subplots(1, 1)\n",
    "\n",
    "    # Param1 is the X-axis, Param 2 is represented as a different curve (color line)\n",
    "    ax.plot(grid_param, scores_mean, label=\"CV-Curve\")\n",
    "\n",
    "    ax.set_title(\"Grid Search Scores\", fontsize=20, fontweight='bold')\n",
    "    ax.set_xlabel(name_param, fontsize=16)\n",
    "    ax.set_ylabel('CV Average Score', fontsize=16)\n",
    "    ax.legend(loc=\"best\", fontsize=15)\n",
    "    ax.grid('on')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build both Scaling and Scale & PCA functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(train_x, test_x):\n",
    "    scaler = StandardScaler()\n",
    "    # Don't cheat - fit only on training data\n",
    "    scaler.fit(train_x)\n",
    "    X_train = scaler.transform(train_x)\n",
    "    # apply same transformation to test data\n",
    "    X_test = scaler.transform(test_x)\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "def scale_and_pca(train_x, test_x):\n",
    "    scaled_train_x, scaled_test_x = scale(train_x, test_x)\n",
    "    pr_comp = PCA(n_components=0.99, svd_solver='full')\n",
    "    pr_comp.fit(scaled_train_x)\n",
    "    return pr_comp.transform(scaled_train_x), pr_comp.transform(scaled_test_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN MODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.pyimagesearch.com/2016/08/15/how-to-tune-hyperparameters-with-python-and-scikit-learn/\n",
    "def tune_knn(train_x, train_y, test_x, test_y):\n",
    "    # Get Number of features\n",
    "    rows = np.shape(train_x)[0]\n",
    "    print(\"There are \" + str(rows) + \" features\")\n",
    "\n",
    "    if rows > 101:\n",
    "        rows = 101\n",
    "    else:\n",
    "        rows = int((rows/2) - 1)\n",
    "\n",
    "    print(\"Highest value of k is: \" + str(rows) + \" features\")\n",
    "    n = np.arange(3, rows, 2)\n",
    "    param_grid = {'n_neighbors': n}\n",
    "    model = KNeighborsClassifier()\n",
    "    start = time.time()\n",
    "    # tune the hyper parameters via a randomized search\n",
    "    best_knn = GridSearchCV(model, param_grid, n_jobs=-1)\n",
    "    best_knn.fit(train_x, train_y)\n",
    "\n",
    "    # Plot the CV-Curve\n",
    "    plot_grid_search(best_knn.cv_results_, n, 'n_neighbors')\n",
    "\n",
    "    # evaluate the best randomized searched model on the testing data\n",
    "    print(\"[INFO] KNN-Best Parameters: \" + str(best_knn.best_params_))\n",
    "    print(\"[INFO] randomized search took {:.2f} seconds\".format(time.time() - start))\n",
    "    print(\"Training Score is: \" + str(best_knn.score(train_x, train_y)))\n",
    "    predictions = best_knn.predict(test_x)\n",
    "    print(\"Testing Score is: \" + str(accuracy_score(test_y, predictions)))\n",
    "    classification_report(test_y, predictions, target_names=best_knn.classes_)\n",
    "    top(best_knn, test_x, test_y, 3)\n",
    "    return best_knn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_linear(train_x, train_y, test_x, test_y):\n",
    "    start = time.time()\n",
    "    n = np.logspace(-3, 3)\n",
    "    param_grid = {'C': n}\n",
    "    log = LogisticRegression(warm_start=False)\n",
    "    log_model = GridSearchCV(log, param_grid, n_jobs=-1)\n",
    "    log_model.fit(train_x, train_y)\n",
    "    plot_grid_search(log_model.cv_results_, n, 'C')\n",
    "\n",
    "    print(\"[INFO] Logistic Regression-Best Parameters: \" + str(log_model.best_params_))\n",
    "    print(\"[INFO] randomized search took {:.2f} seconds\".format(time.time() - start))\n",
    "    print(\"Training Score is: \" + str(log_model.score(train_x, train_y)))\n",
    "\n",
    "    predictions = log_model.predict(test_x)\n",
    "    print(\"Testing Score is: \" + str(accuracy_score(test_y, predictions)))\n",
    "    classification_report(test_y, predictions, target_name=log_model.classes_)\n",
    "    top(log_model, test_x, test_y, 3)\n",
    "    return log_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_forest(train_x, train_y, test_x, test_y):\n",
    "    start_time = time.time()\n",
    "    best_forest = tune_forest(train_x, train_y)\n",
    "    print(\"--- Best Parameter Random Forest Time: %s seconds ---\" % (time.time() - start_time))\n",
    "    print(\"Best Random Forest Parameters: \" + str(best_forest.best_params_))\n",
    "    print(\"Training Mean Test Score: \" + str(best_forest.score(train_x, train_y)))\n",
    "    y_hat = best_forest.predict(test_x)\n",
    "    print(\"Testing Mean Test Score \" + str(metrics.accuracy_score(test_y, y_hat)))\n",
    "\n",
    "    # for i in range(len(test_y)):\n",
    "    #    print(predictions[i])\n",
    "\n",
    "    top(best_forest, test_x, test_y)\n",
    "    return best_forest\n",
    "\n",
    "\n",
    "# Citation:\n",
    "# https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html#sphx-glr-auto-examples-model-selection-plot-randomized-search-py\n",
    "# https://towardsdatascience.com/random-forest-in-python-24d0893d51c0\n",
    "def tune_forest(train_features, train_labels):\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = np.arange(10, 510, 10)\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = np.arange(3, 20, 1)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = np.arange(5, 20, 1)\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = np.arange(5, 20, 1)\n",
    "\n",
    "    random_grid = {\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_features': max_features,\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'min_samples_leaf': min_samples_leaf,\n",
    "        }\n",
    "\n",
    "    # Step 1: Use the random grid to search for best hyper parameters\n",
    "    # First create the base model to tune\n",
    "    rf = RandomForestClassifier(warm_start=False)\n",
    "    rf_random = RandomizedSearchCV(estimator=rf, param_distributions={'n_estimators': n_estimators},\n",
    "                                   n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "    rf_random.fit(train_features, train_labels)\n",
    "    plot_grid_search(rf_random.cv_results_, n_estimators, 'n_estimators')\n",
    "\n",
    "    rf = RandomForestClassifier(warm_start=False)\n",
    "    rf_random = RandomizedSearchCV(estimator=rf, param_distributions={'max_features': max_features},\n",
    "                                   n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "    rf_random.fit(train_features, train_labels)\n",
    "    plot_grid_search(rf_random.cv_results_, max_features, 'max_features')\n",
    "\n",
    "    rf = RandomForestClassifier(warm_start=False)\n",
    "    rf_random = RandomizedSearchCV(estimator=rf, param_distributions={'max_depth': max_depth},\n",
    "                                   n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "    rf_random.fit(train_features, train_labels)\n",
    "    plot_grid_search(rf_random.cv_results_, max_depth, 'max_depth')\n",
    "\n",
    "    rf = RandomForestClassifier(warm_start=False)\n",
    "    rf_random = RandomizedSearchCV(estimator=rf, param_distributions={'min_samples_split': min_samples_split},\n",
    "                                   n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "    rf_random.fit(train_features, train_labels)\n",
    "    plot_grid_search(rf_random.cv_results_, min_samples_split, 'min_samples_split')\n",
    "\n",
    "    rf = RandomForestClassifier(warm_start=False)\n",
    "    rf_random = RandomizedSearchCV(estimator=rf, param_distributions={'min_samples_leaf': min_samples_leaf},\n",
    "                                   n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "    rf_random.fit(train_features, train_labels)\n",
    "    plot_grid_search(rf_random.cv_results_, min_samples_leaf, 'min_samples_leaf')\n",
    "\n",
    "    # -----------------LAST STEP!-------------------\n",
    "    # Random search of parameters, using 3 fold cross validation,\n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n",
    "                                   n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "    # Fit the random search model\n",
    "    rf_random.fit(train_features, train_labels)\n",
    "    # TODO: IF I ADD MORE \"Features\", by definition I must increase number of estimators!!\n",
    "    return rf_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_alpha.html#sphx-glr-auto-examples-neural-networks-plot-mlp-alpha-py\n",
    "# http://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_training_curves.html#sphx-glr-auto-examples-neural-networks-plot-mlp-training-curves-py\n",
    "def get_brain(train_x, train_y, test_x, test_y):\n",
    "    start_time = time.time()\n",
    "    clf = tune_brain(train_x, train_y)\n",
    "    print(\"--- Best Parameter NN Generation: %s seconds ---\" % (time.time() - start_time))\n",
    "    # Print Training and Test Error\n",
    "    print(\"Best NN Parameters: \" + str(clf.get_params()))\n",
    "    print(\"Training Mean Test Score: \" + str(clf.score(train_x, train_y)))\n",
    "    predictions = clf.predict(test_x)\n",
    "    print(\"Testing Mean Test Score: \" + str(accuracy_score(test_y, predictions)))\n",
    "    classification_report(test_y, predictions, target_names=clf.classes_)\n",
    "    top(clf, test_x, test_y)\n",
    "    return clf\n",
    "\n",
    "\n",
    "# Note alpha needs to grow exponentially!\n",
    "def tune_brain(train_x, train_y):\n",
    "    # want to go from 0.001 to 1, but on exponential scale!\n",
    "    alphas = np.logspace(start=-5, stop=0, endpoint=True, num=5)\n",
    "    hidden_layer = np.arange(3, 10, 1)\n",
    "    solvers = {'lbfgs', 'adam'}\n",
    "    param_grid = {'alpha': alphas, 'hidden_layer_sizes': hidden_layer, 'solver':solvers}\n",
    "\n",
    "    model = MLPClassifier(warm_start=False)\n",
    "    # ----alpha----\n",
    "    clf = GridSearchCV(model, param_grid, n_jobs=-1, cv=3)\n",
    "    clf.fit(train_x, train_y)\n",
    "    plot_grid_search(clf.cv_results_, alphas, 'alpha')\n",
    "    # ----hidden layer----\n",
    "    clf = GridSearchCV(model, param_grid, n_jobs=-1, cv=3)\n",
    "    clf.fit(train_x, train_y)\n",
    "    plot_grid_search(clf.cv_results_, hidden_layer, 'hidden_layer')\n",
    "    # ---solvers----\n",
    "    clf = GridSearchCV(model, param_grid, n_jobs=-1, cv=3)\n",
    "    clf.fit(train_x, train_y)\n",
    "    plot_grid_search(clf.cv_results_, solvers, 'solver')\n",
    "    # ----------------Final---------------------\n",
    "    clf = GridSearchCV(model, param_grid, n_jobs=-1, cv=3)\n",
    "    clf.fit(train_x, train_y)\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM (Linear and RBF) Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Default is 10...\n",
    "def svc_rbf_param_selection(x, y, n_folds=2):\n",
    "    c = np.arange(0.01, 1, 0.01)\n",
    "    gammas = np.arange(0.01, 1, 0.01)\n",
    "    param_grid = {'C': c, 'gamma': gammas}\n",
    "    model = svm.SVC(kernel='rbf')\n",
    "\n",
    "    # Test with just cost...\n",
    "    rbf_search = GridSearchCV(model, param_grid={'C': c}, cv=n_folds, n_jobs=-1)\n",
    "    rbf_search.fit(x, y)\n",
    "    plot_grid_search(rbf_search.cv_results_, c, 'C')\n",
    "\n",
    "    # Test with just gamma\n",
    "    rbf_search = GridSearchCV(model, param_grid={'gamma': gammas}, cv=n_folds, n_jobs=-1)\n",
    "    rbf_search.fit(x, y)\n",
    "    plot_grid_search(rbf_search.cv_results_, gammas, 'gamma')\n",
    "\n",
    "    # FINAL STEP\n",
    "    rbf_search = GridSearchCV(model, param_grid=param_grid, cv=n_folds, n_jobs=-1)\n",
    "    rbf_search.fit(x, y)\n",
    "    return rbf_search\n",
    "\n",
    "\n",
    "# Default is 10...\n",
    "# Should take about 6 minutes\n",
    "def svc_linear_param_selection(x, y, n_folds=2):\n",
    "    c = np.arange(0.01, 1, 0.01)\n",
    "    param_grid = {'C': c}\n",
    "    model = svm.SVC(kernel='linear')\n",
    "    svm_line = GridSearchCV(model, param_grid, cv=n_folds, n_jobs=-1)\n",
    "    svm_line.fit(x, y)\n",
    "    plot_grid_search(svm_line.cv_results_, c, 'C')\n",
    "    return svm_line\n",
    "\n",
    "\n",
    "# This is always TRUTH, PREDICTION\n",
    "# http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "def svm_linear(train_x, train_y, test_x, test_y):\n",
    "    start_time = time.time()\n",
    "    svm_line = svc_linear_param_selection(train_x, train_y)\n",
    "    print(\"--- Best Parameter Linear SVM: %s seconds ---\" % (time.time() - start_time))\n",
    "    print(\"Best Linear Parameters: \" + str(svm_line.best_params_))\n",
    "    print(\"Linear SVM, Training Mean Test Score: \" + str(svm_line.score(train_x, train_y)))\n",
    "    predictions = svm_line.predict(test_x)\n",
    "    print(\"Linear SVM, Testing Mean Test Score: \" + str(accuracy_score(test_y, predictions)))\n",
    "    classification_report(test_y, predictions, labels=svm_line.classes_)\n",
    "    # for i in range(len(test_y)):\n",
    "    #    print(predictions[i])\n",
    "    top(svm_line, test_x, test_y, 3)\n",
    "    return svm_line\n",
    "\n",
    "\n",
    "def svm_rbf(train_x, train_y, test_x, test_y):\n",
    "    start_time = time.time()\n",
    "    svm_radial = svc_rbf_param_selection(train_x, train_y)\n",
    "    print(\"--- Best Parameter RBF: %s seconds ---\" % (time.time() - start_time))\n",
    "    print(\"Best RBF Parameters: \" + str(svm_radial.best_params_))\n",
    "    print(\"RBF SVM, Training Mean Test Score: \" + str(svm_radial.score(train_x, train_y)))\n",
    "    y_hat = svm_radial.predict(test_x)\n",
    "    print(\"RBF SVM, Testing Mean Test Score \" + str(accuracy_score(test_y, y_hat)))\n",
    "    classification_report(test_y, y_hat, target_names=svm_radial.classes_)\n",
    "    top(svm_radial, test_x, test_y, 3)\n",
    "    return svm_radial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA/QDA\n",
    "- Ask Professor Verma how to fix Collinearity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminant_line(train_x, train_y, test_x, test_y):\n",
    "    lda = LinearDiscriminantAnalysis(solver=\"svd\", store_covariance=True)\n",
    "    lda.fit(train_x, train_y)\n",
    "    print(\"Training Score (LDA): \" + str(lda.score(train_x, train_y)))\n",
    "    predictions = lda.predict(test_x)\n",
    "    print(\"Prediction Score is (LDA): \" + str(accuracy_score(test_y, predictions)))\n",
    "\n",
    "    # for i in range(len(test_y)):\n",
    "    #    print(predictions[i])\n",
    "    classification_report(test_y, predictions, target_names=lda.classes_)\n",
    "    top(lda, test_x, test_y, 3)\n",
    "    return lda\n",
    "\n",
    "\n",
    "def discriminant_quad(train_x, train_y, test_x, test_y):\n",
    "    qda = QuadraticDiscriminantAnalysis(store_covariance=True)\n",
    "    qda.fit(train_x, train_y)\n",
    "    print(\"Training Score is (QDA): \" + str(qda.score(train_x, train_y)))\n",
    "    predictions = qda.predict(test_x)\n",
    "    print(\"Prediction Score is (QDA): \" + str(accuracy_score(test_y, predictions)))\n",
    "\n",
    "    # for i in range(len(test_y)):\n",
    "    #    print(predictions[i])\n",
    "    classification_report(test_y, predictions, target_names=qda.classes_)\n",
    "    top(qda, test_x, test_y, extra_rooms=1)\n",
    "    return qda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/auto_examples/calibration/plot_calibration.html#sphx-glr-auto-examples-calibration-plot-calibration-py\n",
    "def naive_bayes(x_train, y_train, x_test, y_test):\n",
    "    # Gaussian Naive-Bayes with no calibration\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(x_train, y_train)  # GaussianNB itself does not support sample-weights\n",
    "    prob_pos_clf = clf.predict_proba(x_test)[:, 1]\n",
    "\n",
    "    # Gaussian Naive-Bayes with isotonic calibration\n",
    "    clf_isotonic = CalibratedClassifierCV(clf, cv=2, method='isotonic')\n",
    "    clf_isotonic.fit(x_train, y_train)\n",
    "    prob_pos_isotonic = clf_isotonic.predict_proba(x_test)[:, 1]\n",
    "\n",
    "    # Gaussian Naive-Bayes with sigmoid calibration\n",
    "    clf_sigmoid = CalibratedClassifierCV(clf, cv=2, method='sigmoid')\n",
    "    clf_sigmoid.fit(x_train, y_train)\n",
    "    prob_pos_sigmoid = clf_sigmoid.predict_proba(x_test)[:, 1]\n",
    "\n",
    "    print(\"Brier scores: (the smaller the better)\")\n",
    "\n",
    "    clf_score = brier_score_loss(y_test, prob_pos_clf)\n",
    "    print(\"No calibration: %1.3f\" % clf_score)\n",
    "    classification_report(y_test, prob_pos_clf, target_names=clf.classes_)\n",
    "\n",
    "    clf_isotonic_score = brier_score_loss(y_test, prob_pos_isotonic)\n",
    "    print(\"With isotonic calibration: %1.3f\" % clf_isotonic_score)\n",
    "    classification_report(y_test, prob_pos_isotonic, target_names=clf_isotonic.classes_)\n",
    "\n",
    "    clf_sigmoid_score = brier_score_loss(y_test, prob_pos_sigmoid)\n",
    "    print(\"With sigmoid calibration: %1.3f\" % clf_sigmoid_score)\n",
    "    classification_report(y_test, prob_pos_sigmoid, target_names=clf_sigmoid.classes_)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indic(train_x):\n",
    "    pca = PCA(n_components=2)\n",
    "    scaled_x = scale(train_x)\n",
    "    return pca.fit_transform(scaled_x)\n",
    "\n",
    "\n",
    "def k_means(train_x, train_y):\n",
    "    num_classes = len(np.unique(train_x, axis=0))\n",
    "    kmeans = KMeans(n_clusters=num_classes)\n",
    "    kmeans.fit(train_x)\n",
    "\n",
    "    # centroids = kmeans.cluster_centers_\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    # print(\"centroids\")\n",
    "    # print(centroids)\n",
    "\n",
    "    # Project High dimensional data to 2-D\n",
    "    # https://stackoverflow.com/questions/27930413/how-to-plot-a-multi-dimensional-data-point-in-python\n",
    "    pr_comp = indic(train_x)\n",
    "    x = pr_comp[:, 0]\n",
    "    y = pr_comp[:, 1]\n",
    "    plt.title(\"Clusters\")\n",
    "\n",
    "    colors = cm.rainbow(np.linspace(0, 1, num_classes))\n",
    "    rows = np.shape(train_x)[0]\n",
    "\n",
    "    # Print Label and feature\n",
    "    for i in range(rows):\n",
    "        print(str(labels[i]) + \" \" + str(train_y[i]))\n",
    "        plt.plot(x[i], y[i], color=colors[labels[i]], marker='o', linestyle='-', markersize=10)\n",
    "\n",
    "    # centroids = indic(centroids)\n",
    "    # plt.scatter(centroids[:, 0], centroids[:, 1], marker=\"X\", s=150, linewidths=5, zorder=10)\n",
    "\n",
    "    # Create legend dictionary...\n",
    "    legend_dict = {}\n",
    "    for i in range(len(labels)):\n",
    "        legend_dict[labels[i]] = colors[labels[i]]\n",
    "\n",
    "    patchlist = []\n",
    "    for key in legend_dict:\n",
    "        data_key = mpatches.Patch(color=legend_dict[key], label=key)\n",
    "        patchlist.append(data_key)\n",
    "\n",
    "    plt.legend(loc='best', handles=patchlist)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Overall Purpose...\n",
    "# I will have 3 Classifiers\n",
    "# Wifi, Bluetooth and Misc. Rooms\n",
    "# I can combined them using the Voting Classifier\n",
    "# Reference: http://scikit-learn.org/stable/modules/ensemble.html\n",
    "# I am assuming the three input classifiers are fitted...\n",
    "def combined_classifier(room_clf, wifi_clf, blue_clf, train_x, train_y):\n",
    "    # The type of classifier will depend on preliminary results...\n",
    "    clf = VotingClassifier(estimators=[('dt', room_clf), ('knn', wifi_clf),\n",
    "                                       ('svc', blue_clf)], voting='soft', weights=[2, 1, 2])\n",
    "    clf.fit(train_x, train_y)\n",
    "    top(clf, test_x, test_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_set(filename, isnumeric=False):\n",
    "    x = np.genfromtxt(filename, delimiter=',', skip_header=1)\n",
    "    if isnumeric:\n",
    "        y = x[:, 0]\n",
    "    else:\n",
    "        y = np.genfromtxt(filename, delimiter=',', skip_header=1, dtype=str)\n",
    "        y = y[:, 0]\n",
    "    # x must delete first column which is the label\n",
    "    x = x[:, 1:]\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS IS THE MAIN METHOD!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_x, blue_y = read_data_set('./blue.csv')\n",
    "wifi_x, wifi_y = read_data_set('./wifi.csv')\n",
    "\n",
    "# Build your CV sets here\n",
    "blue_train_x, blue_train_y, blue_test_x, blue_test_y = get_cv_set(blue_x, blue_y)\n",
    "wifi_train_x, wifi_train_y, wifi_test_x, wifi_test_y = get_cv_set(wifi_x, wifi_y)\n",
    "\n",
    "# Have a Scaled and PCA form of your data\n",
    "fixed_blue_train_x, fixed_blue_test_x = scale_and_pca(blue_train_x, blue_test_x)\n",
    "fixed_wifi_train_x, fixed_wifi_test_x = scale_and_pca(wifi_train_x, wifi_test_x)\n",
    "\n",
    "#-------------Bayes-------------\n",
    "blue_clf = naive_bayes(blue_train_x, blue_train_y, blue_test_x, blue_train_y)\n",
    "wifi_clf = naive_bayes(wifi_train_x, wifi_train_y, wifi_test_x, wifi_train_y)\n",
    "\n",
    "#-------------LDA/QDA-----------\n",
    "blue_lda = discriminant_line(fixed_blue_train_x, blue_train_y, fixed_blue_test_x, blue_test_y)\n",
    "blue_qda = discriminant_quad(blue_train_x, blue_train_y, blue_test_x, blue_test_y)\n",
    "\n",
    "wifi_lda = discriminant_line(fixed_wifi_train_x, wifi_train_y, fixed_wifi_test_x, wifi_test_y)\n",
    "wifi_qda = discriminant_quad(wifi_train_x, wifi_train_y, wifi_test_x, wifi_test_y)\n",
    "\n",
    "#-------------KNN--------------\n",
    "blue_knn = tune_knn(blue_train_x, blue_train_y, blue_test_x, blue_test_y)\n",
    "wifi_knn = tune_knn(wifi_train_x, wifi_train_y, wifi_test_x, wifi_test_y)\n",
    "\n",
    "#-------------Logistic Regression-----------\n",
    "blue_clf = logistic_linear(blue_train_x, blue_train_y, blue_test_x, blue_test_y)\n",
    "wifi_clf = logistic_linear(wifi_train_x, wifi_train_y, wifi_test_x, wifi_test_y)\n",
    "    \n",
    "#-------------Neural Network---------------\n",
    "blue_brain = get_brain(blue_train_x, blue_train_y, blue_test_x, blue_test_y)\n",
    "wifi_brain = get_brain(wifi_train_x, wifi_train_y, wifi_test_x, wifi_test_y)\n",
    "\n",
    "#-------------Random Forest----------------\n",
    "blue_forest = get_forest(blue_train_x, blue_train_y, blue_test_x, blue_test_y)\n",
    "wifi_forest = get_forest(wifi_train_x, wifi_train_y, wifi_test_x, wifi_test_y)    \n",
    "\n",
    "#-------SVM-----------------\n",
    "blue_clf = svm_linear(blue_train_x, blue_train_y, blue_test_x, blue_test_y)\n",
    "blue_clf_rbf = svm_rbf(blue_train_x, blue_train_y, blue_test_x, blue_test_y)\n",
    "\n",
    "wifi_clf = svm_linear(wifi_train_x, wifi_train_y, wifi_test_x, wifi_test_y)\n",
    "wifi_clf_rbf = svm_rbf(wifi_train_x, wifi_train_y, wifi_test_x, wifi_test_y)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
