{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python sci-kit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Google Collab, Tune and Train your classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "import time\n",
    "from joblib import dump, load\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_tree(train_x, train_y, n_fold=10, slow=True, n_iter_search=10):\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = np.arange(5, 20, 1)\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = np.arange(5, 20, 1)\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = np.arange(3, 20, 1)\n",
    "    \n",
    "    random_grid = {\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'min_samples_leaf': min_samples_leaf,\n",
    "        'max_depth': max_depth\n",
    "    }\n",
    "\n",
    "    if slow:\n",
    "        tree = GridSearchCV(estimator=DecisionTreeClassifier(),\n",
    "                                    param_grid=random_grid,\n",
    "                                    cv=n_fold, verbose=2, n_jobs=-1)\n",
    "    else:\n",
    "        tree = RandomizedSearchCV(estimator=DecisionTreeClassifier(),\n",
    "                                          param_distributions=random_grid,\n",
    "                                          cv=n_fold, n_iter=n_iter_search, n_jobs=-1)\n",
    "    rf_min_split.fit(train_features, train_labels)\n",
    "    # plot_grid_search(rf_min_split.cv_results_, min_samples_split, 'min_samples_split')\n",
    "    # plot_grid_search(rf_min_leaf.cv_results_, min_samples_leaf, 'min_samples_leaf')\n",
    "    # plot_grid_search(rf_distro.cv_results_, max_depth, 'max_depth')\n",
    "\n",
    "    tree.fit(train_x, train_y)\n",
    "    return tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_knn(train_x, train_y, n_fold=10, slow=True):\n",
    "    # Get Number of features\n",
    "    rows = np.shape(train_x)[0]\n",
    "\n",
    "    if rows > 101:\n",
    "        rows = 101\n",
    "    else:\n",
    "        rows = int((rows/2) - 1)\n",
    "\n",
    "    # print(\"Highest value of k to tune up to is: \" + str(rows) + \" features\")\n",
    "    n = np.arange(3, rows, 2)\n",
    "    start = time.time()\n",
    "    # tune the hyper parameters via a randomized search\n",
    "    if slow:\n",
    "        best_knn = GridSearchCV(estimator=KNeighborsClassifier(), param_grid={'n_neighbors': n},\n",
    "                                n_jobs=-1, cv=n_fold)\n",
    "    else:\n",
    "        best_knn = RandomizedSearchCV(estimator=KNeighborsClassifier(), param_distributions={'n_neighbors': n},\n",
    "                                      n_jobs=-1, cv=n_fold)\n",
    "    best_knn.fit(train_x, train_y)\n",
    "    # Plot the CV-Curve\n",
    "    # plot_grid_search(best_knn.cv_results_, n, 'KNN_n_neighbors')\n",
    "\n",
    "    # evaluate the best randomized searched model on the testing data\n",
    "    print(\"[INFO] KNN-Best Parameters: \" + str(best_knn.best_params_))\n",
    "    print(\"[INFO] Tuning took {:.2f} seconds\".format(time.time() - start))\n",
    "    print(\"[KNN] Training Score is: \" + str(best_knn.score(train_x, train_y)))\n",
    "\n",
    "    with open(\"results.txt\", \"a+\") as my_file:\n",
    "        my_file.write(\"[KNN] KNN-Best Parameters: \" + str(best_knn.best_params_))\n",
    "        my_file.write(\"[KNN] Training Mean Test Score: \" + str(best_knn.score(train_x, train_y)) + '\\n')\n",
    "    return best_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_logistic(train_x, train_y, n_fold=10, slow=True):\n",
    "    start = time.time()\n",
    "    n = np.logspace(-3, 3)\n",
    "    param_grid = {'C': n}\n",
    "    log = LogisticRegression(warm_start=False, max_iter=1000, multi_class='auto', solver='lbfgs')\n",
    "    if slow:\n",
    "        log_model = GridSearchCV(log, param_grid, n_jobs=-1, cv=n_fold, verbose=2)\n",
    "    else:\n",
    "        log_model = RandomizedSearchCV(log, param_grid, n_jobs=-1, cv=n_fold,  verbose=2)\n",
    "    log_model.fit(x, y)\n",
    "    # plot_grid_search(log_model.cv_results_, n, 'Logistic_Regression_Cost')\n",
    "\n",
    "    print(\"[INFO] Logistic Regression-Best Parameters: \" + str(log_model.best_params_))\n",
    "    print(\"[INFO] randomized search took {:.2f} seconds\".format(time.time() - start))\n",
    "    print(\"[Logistic] Training Score is: \" + str(log_model.score(train_x, train_y)))\n",
    "\n",
    "    with open(\"results.txt\", \"a+\") as my_file:\n",
    "        my_file.write(\"[Logistic Regression] Best Parameters: \" + str(log_model.get_params()) + '\\n')\n",
    "        my_file.write(\"[Logistic Regression] Training Mean Test Score: \" +\n",
    "                      str(log_model.score(train_x, train_y)) + '\\n')\n",
    "    return log_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Citation:\n",
    "# https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html#sphx-glr-auto-examples-model-selection-plot-randomized-search-py\n",
    "# https://towardsdatascience.com/random-forest-in-python-24d0893d51c0\n",
    "def tune_forest(train_x, train_y, n_fold=10, slow=True):\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = np.arange(10, 510, 10)\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = np.arange(3, 20, 1)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = np.arange(5, 20, 1)\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = np.arange(5, 20, 1)\n",
    "\n",
    "    random_grid = {\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_features': max_features,\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'min_samples_leaf': min_samples_leaf\n",
    "    }\n",
    "\n",
    "    # Step 1: Use the random grid to search for best hyper parameters\n",
    "    # First create the base model to tune\n",
    "    rf = RandomForestClassifier(warm_start=False, n_estimators=100)\n",
    "    if slow:\n",
    "        tune_rf = GridSearchCV(estimator=rf, param_grid=random_grid, \n",
    "                                    cv=n_fold, n_jobs=-1, verbose=2)\n",
    "    else:\n",
    "        tune_rf = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n",
    "                                         cv=n_fold, n_jobs=-1, verbose=2)\n",
    "    # plot_grid_search(rf_estimate.cv_results_, n_estimators, 'n_estimators')\n",
    "    # plot_grid_search(rf_max.cv_results_, max_features, 'max_features')\n",
    "    # plot_grid_search(rf_distro.cv_results_, max_depth, 'max_depth')\n",
    "    # plot_grid_search(rf_min_split.cv_results_, min_samples_split, 'min_samples_split')\n",
    "    # plot_grid_search(rf_min_leaf.cv_results_, min_samples_leaf, 'min_samples_leaf')\n",
    "    tune_rf.fit(train_x, train_y)\n",
    "    return tune_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with Radial Basis Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svc_rbf_param_selection(x, y, n_folds=10, slow=True):\n",
    "    c = np.arange(0.1, 1, 0.1)\n",
    "    gammas = np.arange(0.1, 1, 0.1)\n",
    "    random_grid = {\n",
    "        'C': c,\n",
    "        'gamma': gammas\n",
    "    }\n",
    "    if slow:\n",
    "        rbf_search = GridSearchCV(svm.SVC(kernel='rbf', gamma='scale'), param_grid=random_grid, cv=n_folds,\n",
    "                                       n_jobs=-1, error_score='raise', verbose=2)\n",
    "    else:\n",
    "        rbf_search = RandomizedSearchCV(svm.SVC(kernel='rbf', gamma='scale'), param_distributions=random_grid,\n",
    "                                             cv=n_folds, n_jobs=-1, error_score='raise', verbose=2)\n",
    "    # plot_grid_search(rbf_search_cost.cv_results_, c, 'SVM_RBF_Cost')\n",
    "    # plot_grid_search(rbf_search_gamma.cv_results_, gammas, 'SVM_RBF_Gamma')\n",
    "    rbf_search.fit(x, y)\n",
    "    return rbf_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Default is 10...\n",
    "def svc_linear_param_selection(train_x, train_y, n_folds=10, slow=False):\n",
    "    c = np.arange(0.1, 1, 0.1)\n",
    "    param_grid = {'C': c}\n",
    "    model = svm.SVC(kernel='linear')\n",
    "    if slow:\n",
    "        svm_line = GridSearchCV(model, param_grid, cv=n_folds, n_jobs=-1, error_score='raise')\n",
    "    else:\n",
    "        svm_line = RandomizedSearchCV(model, param_grid, cv=n_folds, n_jobs=-1, error_score='raise')\n",
    "    svm_line.fit(train_x, train_y)\n",
    "    # plot_grid_search(svm_line.cv_results_, c, 'SVM_Linear_Cost')\n",
    "    return svm_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code to Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def task(train_x, train_y, n_fold=10, slow=False):\n",
    "    tree = tune_tree(train_x, train_y, n_fold, slow)\n",
    "    best_forest = tune_forest(train_x, train_y, n_fold, slow)\n",
    "    knn = get_knn(train_x, train_y, n_fold, slow)\n",
    "    log_model = get_logistic(train_x, train_y, n_fold, slow)\n",
    "    svm_line = svc_linear_param_selection(train_x, train_y, n_fold, slow)\n",
    "    svm_radial = svc_rbf_param_selection(train_x, train_y, n_fold, slow)\n",
    "    # Dump all Classifiers\n",
    "    dump(tree, 'tree.joblib') \n",
    "    dump(best_forest, 'random_forest.joblib')\n",
    "    dump(knn, 'knn.joblib') \n",
    "    dump(log_model, 'logistic.joblib') \n",
    "    dump(svm_line, 'svm_linear.joblib') \n",
    "    dump(svm_radial, 'svm_rbf.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(file, skip_head=True):\n",
    "    if skip_head:\n",
    "        features = np.genfromtxt(file, delimiter=',', skip_header=1, dtype=float, autostrip=True, converters=None)\n",
    "    else:\n",
    "        features = np.genfromtxt(file, delimiter=',', skip_header=0, dtype=float, autostrip=True, converters=None)\n",
    "\n",
    "    if np.isnan(features).any():\n",
    "        if skip_head:\n",
    "            features = np.genfromtxt(file, delimiter=',', skip_header=1, dtype=str, autostrip=True, converters=None)\n",
    "        else:\n",
    "            features = np.genfromtxt(file, delimiter=',', skip_header=0, dtype=str, autostrip=True, converters=None)\n",
    "        classes = features[:, 0]\n",
    "        features = features[:, 1:]\n",
    "        # Now you have NaN in your features, ok now you have issues!\n",
    "        if np.isnan(features).any():\n",
    "            print(\"There are NaNs found in your features at: \" + str(list(map(tuple, np.where(np.isnan(features))))))\n",
    "            exit(0)\n",
    "        else:\n",
    "            features.astype(float)\n",
    "    else:\n",
    "        classes = features[:, 0]\n",
    "        features = features[:, 1:]\n",
    "    return features, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-fc2b4561b498>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "./content/encoded_kddcup.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-5316a6e9b7e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# train_x, train_y = read_data(\"./encoded_kddcup.csv\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./content/encoded_kddcup.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-ec83646513a5>\u001b[0m in \u001b[0;36mread_data\u001b[1;34m(file, skip_head)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_head\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mskip_head\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mautostrip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconverters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mautostrip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconverters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[1;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows)\u001b[0m\n\u001b[0;32m   1549\u001b[0m                 \u001b[0mfhd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rbU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1551\u001b[1;33m                 \u001b[0mfhd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m             \u001b[0mown_fhd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, path, mode)\u001b[0m\n\u001b[0;32m    499\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_file_openers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s not found.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: ./content/encoded_kddcup.csv not found."
     ]
    }
   ],
   "source": [
    "# train_x, train_y = read_data(\"./encoded_kddcup.csv\")\n",
    "train_x, train_y = read_data(\"./content/encoded_kddcup.csv\")\n",
    "task(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
