{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python sci-kit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Google Collab, Tune and Train your classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "import time\n",
    "from joblib import dump, load\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-6eb70800f444>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-6eb70800f444>\"\u001b[1;36m, line \u001b[1;32m21\u001b[0m\n\u001b[1;33m    param_distributions=,\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def tune_tree(train_x, train_y, n_fold=10, slow=True, n_iter_search=10):\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = np.arange(5, 20, 1)\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = np.arange(5, 20, 1)\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = np.arange(3, 20, 1)\n",
    "    \n",
    "    random_grid = {\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'min_samples_leaf': min_samples_leaf,\n",
    "        'max_depth': max_depth\n",
    "    }\n",
    "\n",
    "    if slow:\n",
    "        tree = GridSearchCV(estimator=DecisionTreeClassifier(),\n",
    "                                    param_grid=random_grid,\n",
    "                                    cv=n_fold, verbose=2, n_jobs=-1)\n",
    "    else:\n",
    "        tree = RandomizedSearchCV(estimator=DecisionTreeClassifier(),\n",
    "                                          param_distributions=,\n",
    "                                          cv=n_fold, n_iter=n_iter_search, n_jobs=-1)\n",
    "    rf_min_split.fit(train_features, train_labels)\n",
    "    # plot_grid_search(rf_min_split.cv_results_, min_samples_split, 'min_samples_split')\n",
    "    # plot_grid_search(rf_min_leaf.cv_results_, min_samples_leaf, 'min_samples_leaf')\n",
    "    # plot_grid_search(rf_distro.cv_results_, max_depth, 'max_depth')\n",
    "\n",
    "    tree.fit(train_features, train_labels)\n",
    "    return tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_knn(train_x, train_y, n_fold=10, slow=True):\n",
    "    # Get Number of features\n",
    "    rows = np.shape(train_x)[0]\n",
    "\n",
    "    if rows > 101:\n",
    "        rows = 101\n",
    "    else:\n",
    "        rows = int((rows/2) - 1)\n",
    "\n",
    "    # print(\"Highest value of k to tune up to is: \" + str(rows) + \" features\")\n",
    "    n = np.arange(3, rows, 2)\n",
    "    start = time.time()\n",
    "    # tune the hyper parameters via a randomized search\n",
    "    if slow:\n",
    "        best_knn = GridSearchCV(estimator=KNeighborsClassifier(), param_grid={'n_neighbors': n},\n",
    "                                n_jobs=-1, cv=n_fold)\n",
    "    else:\n",
    "        best_knn = RandomizedSearchCV(estimator=KNeighborsClassifier(), param_distributions={'n_neighbors': n},\n",
    "                                      n_jobs=-1, cv=n_fold)\n",
    "    best_knn.fit(train_x, train_y)\n",
    "    # Plot the CV-Curve\n",
    "    # plot_grid_search(best_knn.cv_results_, n, 'KNN_n_neighbors')\n",
    "\n",
    "    # evaluate the best randomized searched model on the testing data\n",
    "    print(\"[INFO] KNN-Best Parameters: \" + str(best_knn.best_params_))\n",
    "    print(\"[INFO] Tuning took {:.2f} seconds\".format(time.time() - start))\n",
    "    print(\"[KNN] Training Score is: \" + str(best_knn.score(train_x, train_y)))\n",
    "\n",
    "    with open(\"results.txt\", \"a+\") as my_file:\n",
    "        my_file.write(\"[KNN] KNN-Best Parameters: \" + str(best_knn.best_params_))\n",
    "        my_file.write(\"[KNN] Training Mean Test Score: \" + str(best_knn.score(train_x, train_y)) + '\\n')\n",
    "    return best_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_logistic(train_x, train_y, n_fold=10, slow=True):\n",
    "    start = time.time()\n",
    "    n = np.logspace(-3, 3)\n",
    "    param_grid = {'C': n}\n",
    "    log = LogisticRegression(warm_start=False, max_iter=1000, multi_class='auto', solver='lbfgs')\n",
    "    if slow:\n",
    "        log_model = GridSearchCV(log, param_grid, n_jobs=-1, cv=n_fold, verbose=2)\n",
    "    else:\n",
    "        log_model = RandomizedSearchCV(log, param_grid, n_jobs=-1, cv=n_fold,  verbose=2)\n",
    "    log_model.fit(x, y)\n",
    "    # plot_grid_search(log_model.cv_results_, n, 'Logistic_Regression_Cost')\n",
    "\n",
    "    print(\"[INFO] Logistic Regression-Best Parameters: \" + str(log_model.best_params_))\n",
    "    print(\"[INFO] randomized search took {:.2f} seconds\".format(time.time() - start))\n",
    "    print(\"[Logistic] Training Score is: \" + str(log_model.score(train_x, train_y)))\n",
    "\n",
    "    with open(\"results.txt\", \"a+\") as my_file:\n",
    "        my_file.write(\"[Logistic Regression] Best Parameters: \" + str(log_model.get_params()) + '\\n')\n",
    "        my_file.write(\"[Logistic Regression] Training Mean Test Score: \" +\n",
    "                      str(log_model.score(train_x, train_y)) + '\\n')\n",
    "    return log_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Citation:\n",
    "# https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html#sphx-glr-auto-examples-model-selection-plot-randomized-search-py\n",
    "# https://towardsdatascience.com/random-forest-in-python-24d0893d51c0\n",
    "def tune_forest(train_x, train_y, n_fold=10, slow=True):\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = np.arange(10, 510, 10)\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = np.arange(3, 20, 1)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = np.arange(5, 20, 1)\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = np.arange(5, 20, 1)\n",
    "\n",
    "    random_grid = {\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_features': max_features,\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'min_samples_leaf': min_samples_leaf\n",
    "    }\n",
    "\n",
    "    # Step 1: Use the random grid to search for best hyper parameters\n",
    "    # First create the base model to tune\n",
    "    rf = RandomForestClassifier(warm_start=False, n_estimators=100)\n",
    "    if slow:\n",
    "        tune_rf = GridSearchCV(estimator=rf, param_grid=random_grid, \n",
    "                                    cv=n_fold, n_jobs=-1, verbose=2)\n",
    "    else:\n",
    "        tune_rf = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n",
    "                                         cv=n_fold, n_jobs=-1, verbose=2)\n",
    "    # plot_grid_search(rf_estimate.cv_results_, n_estimators, 'n_estimators')\n",
    "    # plot_grid_search(rf_max.cv_results_, max_features, 'max_features')\n",
    "    # plot_grid_search(rf_distro.cv_results_, max_depth, 'max_depth')\n",
    "    # plot_grid_search(rf_min_split.cv_results_, min_samples_split, 'min_samples_split')\n",
    "    # plot_grid_search(rf_min_leaf.cv_results_, min_samples_leaf, 'min_samples_leaf')\n",
    "    tune_rf.fit(train_x, train_y)\n",
    "    return tune_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with Radial Basis Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svc_rbf_param_selection(x, y, n_folds=10, slow=True):\n",
    "    c = np.arange(0.1, 1, 0.1)\n",
    "    gammas = np.arange(0.1, 1, 0.1)\n",
    "    random_grid = {\n",
    "        'C': c,\n",
    "        'gamma': gammas\n",
    "    }\n",
    "    if slow:\n",
    "        rbf_search = GridSearchCV(svm.SVC(kernel='rbf', gamma='scale'), param_grid=random_grid, cv=n_folds,\n",
    "                                       n_jobs=-1, error_score='raise', verbose=2)\n",
    "    else:\n",
    "        rbf_search = RandomizedSearchCV(svm.SVC(kernel='rbf', gamma='scale'), param_distributions=random_grid,\n",
    "                                             cv=n_folds, n_jobs=-1, error_score='raise', verbose=2)\n",
    "    # plot_grid_search(rbf_search_cost.cv_results_, c, 'SVM_RBF_Cost')\n",
    "    # plot_grid_search(rbf_search_gamma.cv_results_, gammas, 'SVM_RBF_Gamma')\n",
    "    rbf_search.fit(x, y)\n",
    "    return rbf_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Default is 10...\n",
    "def svc_linear_param_selection(train_x, train_y, n_folds=10, slow=False):\n",
    "    c = np.arange(0.1, 1, 0.1)\n",
    "    param_grid = {'C': c}\n",
    "    model = svm.SVC(kernel='linear')\n",
    "    if slow:\n",
    "        svm_line = GridSearchCV(model, param_grid, cv=n_folds, n_jobs=-1, error_score='raise')\n",
    "    else:\n",
    "        svm_line = RandomizedSearchCV(model, param_grid, cv=n_folds, n_jobs=-1, error_score='raise')\n",
    "    svm_line.fit(train_x, train_y)\n",
    "    # plot_grid_search(svm_line.cv_results_, c, 'SVM_Linear_Cost')\n",
    "    return svm_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code to Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def task(train_x, train_y, n_fold=10, slow=False):\n",
    "    tree = tune_tree(train_x, train_y, n_fold, slow)\n",
    "    best_forest = tune_forest(train_x, train_y, n_fold, slow)\n",
    "    knn = get_knn(train_x, train_y, n_fold, slow)\n",
    "    log_model = get_logistic(train_x, train_y, n_fold, slow)\n",
    "    svm_line = svc_linear_param_selection(train_x, train_y, n_fold, slow)\n",
    "    svm_radial = svc_rbf_param_selection(train_x, train_y, n_fold, slow)\n",
    "    # Dump all Classifiers\n",
    "    dump(tree, 'tree.joblib') \n",
    "    dump(best_forest, 'random_forest.joblib')\n",
    "    dump(knn, 'knn.joblib') \n",
    "    dump(log_model, 'logistic.joblib') \n",
    "    dump(svm_line, 'svm_linear.joblib') \n",
    "    dump(svm_radial, 'svm_rbf.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(file, skip_head=True):\n",
    "    if skip_head:\n",
    "        features = np.genfromtxt(file, delimiter=',', skip_header=1, dtype=float, autostrip=True, converters=None)\n",
    "    else:\n",
    "        features = np.genfromtxt(file, delimiter=',', skip_header=0, dtype=float, autostrip=True, converters=None)\n",
    "\n",
    "    if np.isnan(features).any():\n",
    "        if skip_head:\n",
    "            features = np.genfromtxt(file, delimiter=',', skip_header=1, dtype=str, autostrip=True, converters=None)\n",
    "        else:\n",
    "            features = np.genfromtxt(file, delimiter=',', skip_header=0, dtype=str, autostrip=True, converters=None)\n",
    "        classes = features[:, 0]\n",
    "        features = features[:, 1:]\n",
    "        # Now you have NaN in your features, ok now you have issues!\n",
    "        if np.isnan(features).any():\n",
    "            print(\"There are NaNs found in your features at: \" + str(list(map(tuple, np.where(np.isnan(features))))))\n",
    "            exit(0)\n",
    "        else:\n",
    "            features.astype(float)\n",
    "    else:\n",
    "        classes = features[:, 0]\n",
    "        features = features[:, 1:]\n",
    "    return features, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_x, train_y = read_data(\"./encoded_kddcup.csv\")\n",
    "train_x, train_y = read_data(\"./content/encoded_kddcup.csv\")\n",
    "task(train_x, train_y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
