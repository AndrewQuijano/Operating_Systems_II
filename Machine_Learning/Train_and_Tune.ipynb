{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python sci-kit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Google Collab, Tune and Train your classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "import time\n",
    "from joblib import dump, load\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tune_tree(train_features, train_labels, n_fold=10, slow=True, n_iter_search=10):\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = np.arange(5, 20, 1)\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = np.arange(5, 20, 1)\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = np.arange(3, 20, 1)\n",
    "\n",
    "    # Tune min split, taken from Random Forest\n",
    "    if slow:\n",
    "        rf_min_split = GridSearchCV(estimator=DecisionTreeClassifier(),\n",
    "                                    param_grid={'min_samples_split': min_samples_split},\n",
    "                                    cv=n_fold, verbose=2, n_jobs=-1)\n",
    "    else:\n",
    "        rf_min_split = RandomizedSearchCV(estimator=DecisionTreeClassifier(),\n",
    "                                          param_distributions={'min_samples_split': min_samples_split},\n",
    "                                          cv=n_fold, n_iter=n_iter_search, n_jobs=-1)\n",
    "    rf_min_split.fit(train_features, train_labels)\n",
    "    # plot_grid_search(rf_min_split.cv_results_, min_samples_split, 'min_samples_split')\n",
    "\n",
    "    # Tune min_sample_leaf, taken from Random Forest\n",
    "    if slow:\n",
    "        rf_min_leaf = GridSearchCV(estimator=DecisionTreeClassifier(),\n",
    "                                   param_grid={'min_samples_leaf': min_samples_leaf},\n",
    "                                   cv=n_fold, n_jobs=-1)\n",
    "    else:\n",
    "        rf_min_leaf = RandomizedSearchCV(estimator=DecisionTreeClassifier(),\n",
    "                                         param_distributions={'min_samples_leaf': min_samples_leaf},\n",
    "                                         cv=n_fold, n_iter=n_iter_search, n_jobs=-1, pre_dispatch='2*n_jobs')\n",
    "    # plot_grid_search(rf_min_leaf.cv_results_, min_samples_leaf, 'min_samples_leaf')\n",
    "\n",
    "    if slow:\n",
    "        rf_distro = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid={'max_depth': max_depth},\n",
    "                                 cv=n_fold, verbose=2, n_jobs=-1, pre_dispatch='2*n_jobs')\n",
    "    else:\n",
    "        rf_distro = RandomizedSearchCV(estimator=DecisionTreeClassifier(), param_distributions={'max_depth': max_depth},\n",
    "                                       cv=n_fold, n_iter=n_iter_search, n_jobs=-1, pre_dispatch='2*n_jobs')\n",
    "    # plot_grid_search(rf_distro.cv_results_, max_depth, 'max_depth')\n",
    "\n",
    "    # Build the classifier with all tuned parameters!\n",
    "    # For the Project I am using this code, I should use entropy\n",
    "    clf = DecisionTreeClassifier(criterion=\"entropy\",\n",
    "                                 max_depth=rf_distro.best_params_['max_depth'],\n",
    "                                 min_samples_split=rf_min_split.best_params_['min_samples_split'],\n",
    "                                 min_samples_leaf=rf_min_leaf.best_params_['min_samples_leaf'])\n",
    "    clf.fit(train_features, train_labels)\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_knn(train_x, train_y, n_fold=10, slow=True):\n",
    "    # Get Number of features\n",
    "    rows = np.shape(train_x)[0]\n",
    "\n",
    "    if rows > 101:\n",
    "        rows = 101\n",
    "    else:\n",
    "        rows = int((rows/2) - 1)\n",
    "\n",
    "    # print(\"Highest value of k to tune up to is: \" + str(rows) + \" features\")\n",
    "    n = np.arange(3, rows, 2)\n",
    "    start = time.time()\n",
    "    # tune the hyper parameters via a randomized search\n",
    "    if slow:\n",
    "        best_knn = GridSearchCV(estimator=KNeighborsClassifier(), param_grid={'n_neighbors': n},\n",
    "                                n_jobs=-1, cv=n_fold)\n",
    "    else:\n",
    "        best_knn = RandomizedSearchCV(estimator=KNeighborsClassifier(), param_distributions={'n_neighbors': n},\n",
    "                                      n_jobs=-1, cv=n_fold)\n",
    "    knn = KNeighborsClassifier(n_neighbors=best_knn.best_params_['n_neighbors'])\n",
    "    knn.fit(train_x, train_y)\n",
    "    # Plot the CV-Curve\n",
    "    # plot_grid_search(best_knn.cv_results_, n, 'KNN_n_neighbors')\n",
    "\n",
    "    # evaluate the best randomized searched model on the testing data\n",
    "    print(\"[INFO] KNN-Best Parameters: \" + str(best_knn.best_params_))\n",
    "    print(\"[INFO] Tuning took {:.2f} seconds\".format(time.time() - start))\n",
    "    print(\"[KNN] Training Score is: \" + str(best_knn.score(train_x, train_y)))\n",
    "\n",
    "    with open(\"results.txt\", \"a+\") as my_file:\n",
    "        my_file.write(\"[KNN] KNN-Best Parameters: \" + str(best_knn.best_params_))\n",
    "        my_file.write(\"[KNN] Training Mean Test Score: \" + str(best_knn.score(train_x, train_y)) + '\\n')\n",
    "    return knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_logistic(train_x, train_y, n_fold=10, slow=True):\n",
    "    start = time.time()\n",
    "    n = np.logspace(-3, 3)\n",
    "    param_grid = {'C': n}\n",
    "    log = LogisticRegression(warm_start=False, max_iter=1000, multi_class='auto', solver='lbfgs')\n",
    "    if slow:\n",
    "        log_model = GridSearchCV(log, param_grid, n_jobs=-1, cv=n_fold, verbose=2)\n",
    "    else:\n",
    "        log_model = RandomizedSearchCV(log, param_grid, n_jobs=-1, cv=n_fold,  verbose=2)\n",
    "        \n",
    "    log_final = LogisticRegression(warm_start=False, max_iter=1000, multi_class='auto', solver='lbfgs', C=log_model.best_params_['C'])\n",
    "    log_final.fit(train_x, train_y)\n",
    "    # plot_grid_search(log_model.cv_results_, n, 'Logistic_Regression_Cost')\n",
    "\n",
    "    print(\"[INFO] Logistic Regression-Best Parameters: \" + str(log_model.best_params_))\n",
    "    print(\"[INFO] randomized search took {:.2f} seconds\".format(time.time() - start))\n",
    "    print(\"[Logistic] Training Score is: \" + str(log_final.score(train_x, train_y)))\n",
    "\n",
    "    with open(\"results.txt\", \"a+\") as my_file:\n",
    "        my_file.write(\"[Logistic Regression] Best Parameters: \" + str(log_model.get_params()) + '\\n')\n",
    "        my_file.write(\"[Logistic Regression] Training Mean Test Score: \" +\n",
    "                      str(log_model.score(train_x, train_y)) + '\\n')\n",
    "    return log_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Citation:\n",
    "# https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html#sphx-glr-auto-examples-model-selection-plot-randomized-search-py\n",
    "# https://towardsdatascience.com/random-forest-in-python-24d0893d51c0\n",
    "def tune_forest(train_features, train_labels, n_fold=10, slow=True):\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = np.arange(10, 510, 10)\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = np.arange(3, 20, 1)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = np.arange(5, 20, 1)\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = np.arange(5, 20, 1)\n",
    "\n",
    "    # random_grid = {\n",
    "    #    'n_estimators': n_estimators,\n",
    "    #    'max_features': max_features,\n",
    "    #    'max_depth': max_depth,\n",
    "    #    'min_samples_split': min_samples_split,\n",
    "    #    'min_samples_leaf': min_samples_leaf,\n",
    "    #    }\n",
    "\n",
    "    # Step 1: Use the random grid to search for best hyper parameters\n",
    "    # First create the base model to tune\n",
    "    rf = RandomForestClassifier(warm_start=False, n_estimators=100)\n",
    "    if slow:\n",
    "        rf_estimate = GridSearchCV(estimator=rf, param_grid={'n_estimators': n_estimators}, \n",
    "                                    cv=n_fold, n_jobs=-1, pre_dispatch=None, verbose=2)\n",
    "    else:\n",
    "        rf_estimate = RandomizedSearchCV(estimator=rf, param_distributions={'n_estimators': n_estimators},\n",
    "                                         cv=n_fold, n_jobs=-1, pre_dispatch='2*n_jobs', verbose=2)\n",
    "    # plot_grid_search(rf_estimate.cv_results_, n_estimators, 'n_estimators')\n",
    "\n",
    "    rf = RandomForestClassifier(warm_start=False, n_estimators=100)\n",
    "    if slow:\n",
    "        rf_max = GridSearchCV(estimator=rf, param_grid={'max_features': max_features},\n",
    "                              cv=n_fold, n_jobs=-1, pre_dispatch=None, verbose=2)\n",
    "    else:\n",
    "        rf_max = RandomizedSearchCV(estimator=rf, param_distributions={'max_features': max_features},\n",
    "                                    cv=n_fold, n_jobs=-1, pre_dispatch='2*n_jobs', verbose=2)\n",
    "    plot_grid_search(rf_max.cv_results_, max_features, 'max_features')\n",
    "\n",
    "    rf = RandomForestClassifier(warm_start=False, n_estimators=100)\n",
    "    if slow:\n",
    "        rf_distro = GridSearchCV(estimator=rf, param_grid={'max_depth': max_depth}, cv=n_fold, n_jobs=-1)\n",
    "    else:\n",
    "        rf_distro = RandomizedSearchCV(estimator=rf, param_distributions={'max_depth': max_depth},\n",
    "                                       cv=n_fold, n_jobs=-1, pre_dispatch='2*n_jobs', verbose=2)\n",
    "    plot_grid_search(rf_distro.cv_results_, max_depth, 'max_depth')\n",
    "\n",
    "    rf = RandomForestClassifier(warm_start=False, n_estimators=100)\n",
    "    if slow:\n",
    "        rf_min_split = GridSearchCV(estimator=rf, param_grid={'min_samples_split': min_samples_split},\n",
    "                                    cv=n_fold, n_jobs=-1, pre_dispatch='2*n_jobs', verbose=2)\n",
    "    else:\n",
    "        rf_min_split = RandomizedSearchCV(estimator=rf, param_distributions={'min_samples_split': min_samples_split}\n",
    "                                          , cv=n_fold, n_jobs=-1, pre_dispatch='2*n_jobs',verbose=2)\n",
    "    plot_grid_search(rf_min_split.cv_results_, min_samples_split, 'min_samples_split')\n",
    "\n",
    "    rf = RandomForestClassifier(warm_start=False, n_estimators=100)\n",
    "    if slow:\n",
    "        rf_min_leaf = GridSearchCV(estimator=rf, param_grid={'min_samples_leaf': min_samples_leaf},\n",
    "                                   cv=n_fold, n_jobs=-1, pre_dispatch=None, verbose=2)\n",
    "    else:\n",
    "        rf_min_leaf = RandomizedSearchCV(estimator=rf, param_distributions={'min_samples_leaf': min_samples_leaf},\n",
    "                                         cv=n_fold, n_jobs=-1, pre_dispatch='2*n_jobs', verbose=2)\n",
    "    plot_grid_search(rf_min_leaf.cv_results_, min_samples_leaf, 'min_samples_leaf')\n",
    "\n",
    "    random_forest = RandomForestClassifier(warm_start=False,\n",
    "                                           n_estimators=rf_estimate.best_params_['n_estimators'],\n",
    "                                           max_features=rf_max.best_params_['max_features'],\n",
    "                                           max_depth=rf_distro.best_params_['max_depth'],\n",
    "                                           min_samples_split=rf_min_split.best_params_['min_samples_split'],\n",
    "                                           min_samples_leaf=rf_min_leaf.best_params_['min_samples_leaf'])\n",
    "    random_forest.fit(train_features, train_labels)\n",
    "    return random_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with Radial Basis Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Default is 10...\n",
    "def svc_rbf_param_selection(x, y, n_folds=10, slow=True):\n",
    "    c = np.arange(0.1, 1, 0.1)\n",
    "    gammas = np.arange(0.1, 1, 0.1)\n",
    "\n",
    "    # Test with just cost...\n",
    "    if slow:\n",
    "        rbf_search_cost = GridSearchCV(svm.SVC(kernel='rbf', gamma='scale'), param_grid={'C': c}, cv=n_folds,\n",
    "                                       n_jobs=-1, error_score='raise', pre_dispatch='2*n_jobs')\n",
    "    else:\n",
    "        rbf_search_cost = RandomizedSearchCV(svm.SVC(kernel='rbf', gamma='scale'), param_distributions={'C': c},\n",
    "                                             cv=n_folds, n_jobs=-1, error_score='raise', pre_dispatch='2*n_jobs')\n",
    "    plot_grid_search(rbf_search_cost.cv_results_, c, 'SVM_RBF_Cost')\n",
    "\n",
    "    # Test with just gamma\n",
    "    if slow:\n",
    "        rbf_search_gamma = GridSearchCV(svm.SVC(kernel='rbf'), param_grid={'gamma': gammas}, cv=n_folds,\n",
    "                                        error_score='raise', pre_dispatch='2*n_jobs')\n",
    "    else:\n",
    "        rbf_search_cost = RandomizedSearchCV(svm.SVC(kernel='rbf', gamma='scale'),\n",
    "                                             param_distributions={'gamma': gammas},\n",
    "                                             cv=n_folds, n_jobs=-1, error_score='raise', pre_dispatch='2*n_jobs')\n",
    "    rbf_search_gamma.fit(x, y)\n",
    "    # plot_grid_search(rbf_search_gamma.cv_results_, gammas, 'SVM_RBF_Gamma')\n",
    "\n",
    "    # FINAL STEP\n",
    "    model = svm.SVC(kernel='rbf', C=rbf_search_cost.best_params_['C'], gamma=rbf_search_gamma.best_params_['gamma'])\n",
    "    model.fit(x, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Default is 10...\n",
    "def svc_linear_param_selection(x, y, n_folds=10, slow=False):\n",
    "    c = np.arange(0.1, 1, 0.1)\n",
    "    param_grid = {'C': c}\n",
    "    model = svm.SVC(kernel='linear')\n",
    "    if slow:\n",
    "        svm_line = GridSearchCV(model, param_grid, cv=n_folds, n_jobs=-1, error_score='raise')\n",
    "    else:\n",
    "        svm_line = RandomizedSearchCV(model, param_grid, cv=n_folds, n_jobs=-1, error_score='raise')\n",
    "    svm_line = svm.SVC(kernel='linear', C=rbf_search_cost.best_params_['C'])\n",
    "    # plot_grid_search(svm_line.cv_results_, c, 'SVM_Linear_Cost')\n",
    "    return svm_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code to Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task(train_x, train_y, n_fold=10, slow=False):\n",
    "    tree = tune_tree(train_x, train_y, n_fold, slow)\n",
    "    best_forest = tune_forest(train_x, train_y, n_fold, slow)\n",
    "    knn = get_knn(train_x, train_y, n_fold, slow)\n",
    "    log_model = get_logistic(train_x, train_y, n_fold, slow)\n",
    "    svm_line = svc_linear_param_selection(train_x, train_y, n_fold, slow)\n",
    "    svm_radial = svc_rbf_param_selection(train_x, train_y, n_fold, slow)\n",
    "    # Dump all Classifiers\n",
    "    dump(tree, 'tree.joblib') \n",
    "    dump(best_forest, 'random_forest.joblib')\n",
    "    dump(knn, 'knn.joblib') \n",
    "    dump(log_model, 'logistic.joblib') \n",
    "    dump(svm_line, 'svm_linear.joblib') \n",
    "    dump(svm_radial, 'svm_rbf.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(file, skip_head=True):\n",
    "    if skip_head:\n",
    "        features = np.genfromtxt(file, delimiter=',', skip_header=1, dtype=float, autostrip=True, converters=None)\n",
    "    else:\n",
    "        features = np.genfromtxt(file, delimiter=',', skip_header=0, dtype=float, autostrip=True, converters=None)\n",
    "\n",
    "    if np.isnan(features).any():\n",
    "        if skip_head:\n",
    "            features = np.genfromtxt(file, delimiter=',', skip_header=1, dtype=str, autostrip=True, converters=None)\n",
    "        else:\n",
    "            features = np.genfromtxt(file, delimiter=',', skip_header=0, dtype=str, autostrip=True, converters=None)\n",
    "        classes = features[:, 0]\n",
    "        features = features[:, 1:]\n",
    "        # Now you have NaN in your features, ok now you have issues!\n",
    "        if np.isnan(features).any():\n",
    "            print(\"There are NaNs found in your features at: \" + str(list(map(tuple, np.where(np.isnan(features))))))\n",
    "            exit(0)\n",
    "        else:\n",
    "            features.astype(float)\n",
    "    else:\n",
    "        classes = features[:, 0]\n",
    "        features = features[:, 1:]\n",
    "    return features, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y = read_data(\"./kddcup.csv\")\n",
    "task(train_x, train_y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
